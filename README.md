# Large Language Model Alignments

## Reward Models
- Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons (1954)
    - *i.e.*, **Bradley-Terry** Preference Model
    - [Paper](https://academic.oup.com/biomet/article/41/3-4/502/231004) | [Wikipedia](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)

- The Analysis of Permutations (1975) & Individual Choice Behavior: A Theoretical Analysis(1961)
    - *i.e.*, **Placett-Luce** Preference Model
    - [Paper1](https://www.jstor.org/stable/2346567) | [Paper2](https://www.jstor.org/stable/2282347) | [Wikipedia](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model#Plackett%E2%80%93Luce_model)


## Policy Optimization
- **Proximal Policy Optimization** Algorithms ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Sample Efficient Reinforcement Learning with **REINFORCE** ()
    - [Paper]() | [GitHub]() | [Webpage]()

## Preference Optimization
- **Direct Preference Optimization**: Your Language Model is Secretly a Reward Model ()
    - [Paper]() | [GitHub]() | [Webpage]()

- **Active Preference Learning** for Large Language Models ()
    - [Paper]() | [GitHub]() | [Webpage]()

## RLHF/RLAIF
- 









<!-- -  ()
    - [Paper]() | [GitHub]() | [Webpage]() -->