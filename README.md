# Large Language Model Alignments

## Reward Models
- Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons (1954)
    - *i.e.*, **Bradley-Terry** Preference Model
    - [Paper](https://academic.oup.com/biomet/article/41/3-4/502/231004) | [Wikipedia](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)

- The Analysis of Permutations (1975) & Individual Choice Behavior: A Theoretical Analysis(1961)
    - *i.e.*, **Placett-Luce** Preference Model
    - [Paper1](https://www.jstor.org/stable/2346567) | [Paper2](https://www.jstor.org/stable/2282347) | [Wikipedia](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model#Plackett%E2%80%93Luce_model)

- **Proximal Policy Optimization** Algorithms ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Sample Efficient Reinforcement Learning with **REINFORCE** ()
    - [Paper]() | [GitHub]() | [Webpage]()

- ReMax ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs (2024)
    - *i.e.*, **RLOO**
    - [Paper](https://aclanthology.org/2024.acl-long.662.pdf) | [HuggingFace](https://huggingface.co/docs/trl/en/rloo_trainer)

- GRPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- REINFORCE++ ()
    - [Paper]() | [GitHub]() | [Webpage]()

- DVPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- PRIME ()
    - [Paper]() | [GitHub]() | [Webpage]()

- SPPD ()
    - [Paper]() | [GitHub]() | [Webpage]()

- UGDA ()
    - [Paper]() | [GitHub]() | [Webpage]()

- **VinePPO**: Unlocking RL Potential for LLM Reasoning Through Refined Credit Assignment ()
    - [Paper](https://arxiv.org/pdf/2410.01679) | [GitHub](https://github.com/McGill-NLP/VinePPO) | [Webpage]()

- L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning ()
    - *i.e.*, **LCPO**
    - [Paper](https://arxiv.org/pdf/2503.04697) | [GitHub](https://github.com/cmu-l3/l1) | [Webpage]()

- **ReST-MCTSâˆ—**: LLM Self-Training via Process Reward Guided Tree Search ()
    - [Paper](https://arxiv.org/pdf/2406.03816) | [GitHub](https://github.com/THUDM/ReST-MCTS) | [Webpage](https://rest-mcts.github.io/)


## Preference Optimization
- **Direct Preference Optimization**: Your Language Model is Secretly a Reward Model ()
    - [Paper]() | [GitHub]() | [Webpage]()

- **Active Preference Learning** for Large Language Models ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability ()
    - *i.e.*, **cDPO**
    - [Paper](https://arxiv.org/pdf/2411.19943) | [GitHub](https://github.com/chenzhiling9954/Critical-Tokens-Matter) | [Webpage]()

- CPL: Critical plan step learning boosts llm generalization in reasoning tasks ()
    - [Paper](https://arxiv.org/pdf/2409.08642)

- Focused-DPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- DAPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- RFTT ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Selective DPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- VC-PPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Light-R1 ()
    - [Paper]() | [GitHub]() | [Webpage]()

- SimPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- DQO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- OREO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- DAPO ()
    - [Paper]() | [GitHub]() | [Webpage]()

- SimpleRL ()
    - [Paper]() | [GitHub]() | [Webpage]()

- DeepScaler ()
    - [Paper]() | [GitHub]() | [Webpage]()

- SimpleRL-Zoo ()
    - [Paper]() | [GitHub]() | [Webpage]()

- X-R1 ()
    - [Paper]() | [GitHub]() | [Webpage]()

- TinyZero ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Open-Reasoner-Zero ()
    - [Paper]() | [GitHub]() | [Webpage]()

- OpenR ()
    - [Paper]() | [GitHub]() | [Webpage]()

- OpenRLHF ()
    - [Paper]() | [GitHub]() | [Webpage]()

- OpenR1 ()
    - [Paper]() | [GitHub]() | [Webpage]()

- Logic-RL ()
    - [Paper]() | [GitHub]() | [Webpage]()

- AReaL ()
    - [Paper]() | [GitHub]() | [Webpage]()

## RLHF/RLAIF
- Deep Reinforcement Learning from Human Preferences (2017)
    - *i.e.*, **Reinforcement Learning with Human Feedback**
    - [Paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf) | [GitHub]() | [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)

- Constitutional ai: Harmlessness from ai feedback (2022)
    - *i.e.*, **Reinforcement Learning with AI Feedback**
    - [Paper](https://arxiv.org/pdf/2212.08073) | [GitHub]() | [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback#Reinforcement_learning_from_AI_feedback)







<!-- 
-  ()
    - [Paper]() | [GitHub]() | [Webpage]()
-->